{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **0. Project History**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **Maintaining a Change Log**\n",
    "\n",
    "    A change log is a file that records all changes made to a project, including bug fixes, new features, and improvements. It's a crucial part of any project's documentation, as it provides a historical record of the project's evolution.\n",
    "\n",
    "##### **Why Maintain a Change Log?**\n",
    "\n",
    "    Tracking Changes: A change log allows you to track all changes made to your project. This is especially useful when working in a team, as it allows everyone to see what changes have been made and why.\n",
    "\n",
    "    Documentation: A change log serves as a form of documentation. It can be used to understand the history of a project, how it evolved, and what the current state of the project is.\n",
    "\n",
    "    Rollback: If a problem arises in a project, a change log can be used to rollback to a previous version of the project.\n",
    "\n",
    "##### **How to Maintain a Change Log**\n",
    "\n",
    "    Keep It Up-to-Date: Whenever you make a change to your project, update the change log. This could be as simple as noting the date, the person making the change, and a brief description of the change.\n",
    "\n",
    "    Use a Consistent Format: Consistency is key when it comes to maintaining a change log. Use a consistent format for all entries, including the date format, the person's name format, and the description format.\n",
    "\n",
    "    Keep It Organized: If your project has a lot of changes, consider organizing your change log. For example, you could group changes by date, by person, or by type of change.\n",
    "\n",
    "    Use Version Control Systems: If you're using a version control system like Git, a change log is usually maintained for you. You can use the commit history to see what changes have been made and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Change Log**\n",
    "\n",
    "\n",
    "<span style=\"color:lime\"> ***2023-11-28 - Amin Rezaei***</span>\n",
    "\n",
    "\n",
    "- Switched off the AutoCalibrate function\n",
    "- Relocated redundant import modules to the file level \n",
    "\n",
    "    ***Imports should be placed at the top of the module, not inside functions, unless there is a specific reason to do so. This makes the code more readable, maintainable, and avoids potential problems with circular imports, name clashes, and performance issues.***\n",
    "\n",
    "- Included a package version control \n",
    "- Implementation of input and output sanity checks\n",
    "- Added a generate CSV result feature for the Step2 results (alg_activity_acti4_v1_1_3.py)\n",
    "\n",
    "\n",
    "<span style=\"color:lime\"> ***2023-01-16 - Amin Rezaei***</span>\n",
    "\n",
    "- Muted Sampling Frequnecy in ChunkActi4Pre_v1_3_3.py - default set to SF=30\n",
    "- Added functions: extract_sensor_id, process_pre_step_data, chunk_data_extended\n",
    "- A simple pdf report of sensor id, sampling frquency, and availble chunks added \n",
    "- Ability to store the chunks of information in csv format under a sensor id name directory added\n",
    "- Added section 7 for validating results and storing non-matching events\n",
    "- Directory structure added\n",
    "\n",
    "<span style=\"color:orange\"> ***2023-02-22 - Sebastian Hørlück***</span>\n",
    "\n",
    "- Adjusted relative imports to fit structure of GitHub repo\n",
    "- Added functionality: Combine pre-chunks within days\n",
    "- Added functionality: Combine classification-chunks for comparison of entire period\n",
    "- Muted pdf generator after step 1\n",
    "- Muted plotting of results after step 2\n",
    "- Converted timestamps from backend to datetime objects when comparing with ground truth\n",
    "- Removed timezone from backend times and ground truth times\n",
    "- Changed alignment of backend and ground truth to be based on `pd.merge_asof(direction=nearest)` using datetime objects from backend and ground truth\n",
    "    - Seems like backend times are pushed one second forward, as matching is much better when subtracting backend time by one second\n",
    "- Add comparison of backend classifications without chunking (does errors come from chunking or backend-specific functions)\n",
    "\n",
    "<span style=\"color:orange\"> ***2024-04-29 Sebastian Hørlück***</span>\n",
    "\n",
    "- Add functionality of multiple sensor input in step 2 (temprorary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **0.1 Project Structure** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workspace is organized as follows:\n",
    "\n",
    "```plaintext\n",
    "Workspace/\n",
    "│\n",
    "├── Acti4_Sandbox (Chunking added).ipynb\n",
    "│\n",
    "├── Extras/\n",
    "│   ├── requirements.txt\n",
    "│   └── sens logo black.png\n",
    "│\n",
    "├── Functions/\n",
    "│   ├── Acti4.py\n",
    "│   ├── alg_activity_acti4_v1_1_3.py\n",
    "│   ├── alg_chunk_acti4pre_v1_1_3.py\n",
    "│   ├── backendfunctions.py\n",
    "│   ├── preprocessing.py\n",
    "│   ├── tools.py\n",
    "│   ├── __init__.py\n",
    "│   └── __pycache__/\n",
    "│\n",
    "├── Sample Dataset/\n",
    "│   ├── Activity_perSecond_2401.csv\n",
    "│   └── export_73-5D.C3_acc-3ax-4g_2023-10-05T220000_2023-10-07T22.00.00.bin\n",
    "│\n",
    "└── results/\n",
    "    └── <sensor_id>/\n",
    "        ├── Activity_Acti4_V_1_1_3/\n",
    "        │   └── activity_chunk_<i+1>.csv\n",
    "        ├── Pre_Acti4_V_1_1_3/\n",
    "        │   └── pre_chunk_<i+1>.csv\n",
    "        ├── non_matching_activity.csv\n",
    "        └── 73-5D.C3_report.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **1. Background**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itY8h7mIGQvH",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### ***About***\n",
    "\n",
    "This Jupyter Notebook provides a sandbox environment for loading raw accelerometer data from the SENS Motion sensor and performing various data analysis tasks. With the help of this notebook, you can easily import and preprocess the data, visualize it, and extract meaningful insights.\n",
    "\n",
    "*Disclaimer*:\n",
    "This notebook is part of the sandbox environmnet developed specifically for SENS Innovation Aps and NFA for the integration of Acti4 algorithm V 1_1_3 into the SENS backend.\n",
    "\n",
    "---\n",
    "\n",
    "#### ***Structure of the notebook***\n",
    "\n",
    "The notebook contains two types of cell:  \n",
    "\n",
    "**Text cells** provide information and can be modified by douple-clicking the cell. You are currently reading the text cell. You can create a new text by clicking `+ Text`.\n",
    "\n",
    "**Code cells** contain code and the code can be modfied by selecting the cell. To execute the cell, move your cursor on the `[ ]`-mark on the left side of the cell (play button appears). Click to execute the cell. After execution is done the animation of play button stops. You can create a new coding cell by clicking `+ Code`.\n",
    "\n",
    "---\n",
    "\n",
    "#### ***Google Colab short intro and features***\n",
    "\n",
    "On the top left side of the notebook you will find the tabs which contain from top to bottom:\n",
    "\n",
    "***Table of contents*** = contains structure of the notebook. Click the content to move quickly between sections.\n",
    "\n",
    "***Find and replace*** = find and replace tool allows you to find and replace text/variables in selected items/cells within the entire Jupyter Notebook .\n",
    "\n",
    "***Variables*** = This feature allows you to inspect and manage variables within your Colab notebook.\n",
    "\n",
    "***Secrets*** = This feature allows you to safely store your private keys, such as your [SENS](https://ask.for.link.dk/r/login) API tokens, in Colab! Values stored in Secrets are private, visible only to you and the notebooks you select.\n",
    "\n",
    "***Files*** =  This feature allows you to manage and access files within your Colab environment. It provides an interface for uploading, downloading, and organizing files, which can be useful for various tasks, including data manipulation and storage..\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Remember that all uploaded files are purged after changing the runtime.** However, all files saved in Google Drive will remain. <u>You do not need to use the Mount Drive-button; your Google Drive can be connected if you run section 1.4.</u>\n",
    "\n",
    "**Note:** If you wish to proceed with the pre-loaded \"sample data\", you do not need to upload anything in the **Section 2. Load your dataset**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxYd-1C4GQvH",
    "tags": []
   },
   "source": [
    "## **2. Preparing the environment**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxTtViIcGQvI"
   },
   "source": [
    "To get started, make sure you have the necessary dependencies installed and loaded.\n",
    "\n",
    "**Tip:** For similar results and smooth experience, make sure to run the cells in chronological order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLnEFP4LGQvI",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **2.1 Install key dependencies**\n",
    "---\n",
    "<font size = 4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrqMPIovGQvK"
   },
   "source": [
    "### **2.2 Load key dependencies**\n",
    "---\n",
    "<font size = 4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1846,
     "status": "ok",
     "timestamp": 1700479366924,
     "user": {
      "displayName": "Amin Rezaei",
      "userId": "03247382606058045083"
     },
     "user_tz": -60
    },
    "id": "KLomjHlbGQvL",
    "outputId": "82c879dd-6dac-4ffc-a62e-dd2d6806dc37",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------------------+\n",
      "| Library | Version | Installation Status |\n",
      "+---------+---------+---------------------+\n",
      "|  Python |  3.11.0 |    Pre Installed    |\n",
      "|  NumPy  |  1.24.4 |      Installed      |\n",
      "|  Pandas |  2.0.3  |      Installed      |\n",
      "|  SciPy  |  1.10.1 |      Installed      |\n",
      "+---------+---------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "import platform\n",
    "import warnings\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "from datetime import timedelta\n",
    "from itertools import product\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from functions.preprocessing import read_bin\n",
    "from alg_chunk_motus_pre_v2_0_0 import ChunkMotusPre_v2_0_0\n",
    "from alg_activity_motus_v2_0_0_ErgoConnect import ActivityMotus_v2_0_0_ErgoConnect\n",
    "from alg_activity_motus_v1_2_0 import ActivityMotus_v1_2_0\n",
    "from functions.BackendExtras.tools import (\n",
    "    sanity_check_pre,\n",
    "    time_sanity,\n",
    "    acti4pre_csv,\n",
    "    activity_motus_csv,\n",
    "    plot_activity_classification,\n",
    "    display_dataframe_info,\n",
    "    extract_sensor_id,\n",
    "    generate_pdf,\n",
    "    chunk_data_extended,\n",
    "    motuspre_merge_outputs,\n",
    "    process_pre_step_data,\n",
    "    extract_id_placements,\n",
    "    get_chunk_start_idx,\n",
    ")\n",
    "\n",
    "from bokeh.layouts import column\n",
    "from bokeh.models import ColumnDataSource, RangeTool\n",
    "from bokeh.plotting import figure, show, output_file, save\n",
    "\n",
    "# Setup autoreload of modules, so kernel does not have to be restarted when import files are changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Define the list of required dependencies\n",
    "dependencies = [\n",
    "    (\n",
    "        \"Python\",\n",
    "        platform.python_version(),\n",
    "        \"Installed\" if \"python\" in sys.modules else \"Pre Installed\",\n",
    "    ),\n",
    "    (\n",
    "        \"NumPy\",\n",
    "        np.__version__,\n",
    "        \"Installed\" if \"numpy\" in sys.modules else \"Not Installed\",\n",
    "    ),\n",
    "    (\n",
    "        \"Pandas\",\n",
    "        pd.__version__,\n",
    "        \"Installed\" if \"pandas\" in sys.modules else \"Not Installed\",\n",
    "    ),\n",
    "    (\n",
    "        \"SciPy\",\n",
    "        scipy.__version__,\n",
    "        \"Installed\" if \"scipy\" in sys.modules else \"Not Installed\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create a table to display package information\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Library\", \"Version\", \"Installation Status\"]\n",
    "\n",
    "# Set the max width\n",
    "table.max_width = 15\n",
    "\n",
    "for dependency in dependencies:\n",
    "    table.add_row(dependency)\n",
    "\n",
    "# Print the table\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYrfsaH8GQvS"
   },
   "source": [
    "## **3. Load your dataset**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiurQDhoHea4"
   },
   "source": [
    "### **3.1 Handling your data**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Downloading data***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKk7peORGQvS"
   },
   "source": [
    "Data export is handled through the [web application](https://app.sens.dk/r/login). We recommend using a browser like Mozilla Firefox on your computer.\n",
    "\n",
    "Exporting data can be achieve through various ways as listed below, however, to make use of this notebook, you need to download the **<u>raw data</u>** in <u>.bin</u> format.\n",
    "\n",
    "* PDF files\n",
    "* Raw data (in .bin or .hex format)\n",
    "* CSV file from accelerometer data\n",
    "* CSV format directly from the Patient Overview\n",
    "\n",
    "If you need guidance and introduction to our **<u>Web App</u>**, please visit our [Support page](https://support.sens.dk/hc/en-us/articles/8001330518429-Data-export) for an in-depth explanation on data retrival from the [Web App](https://app.sens.dk/r/login).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZmV9mIwGQvT"
   },
   "source": [
    "##### ***Setting the path to your data***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**`raw_acc_bin`:** The raw_acc_data should be povided with the path to your data. To find the paths of the folders containing the respective datasets, go to your Files on the left of the notebook, navigate to the folder containing your files and copy the path by right-clicking on the folder, **Copy path** and pasting it into the right box below.\n",
    "\n",
    "***Note*** If you do not wish to use files from your Google Drive, you can simply drag and drop your file to the temporary drive that has been allocated to this session. Any files uploaded to your temporary colab drive will be purged once you close the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set folders\n",
    "src_folder = \"U:\\\\DI\\\\MOTUS\\\\Diverse kode tests\\\\BackendV2_0_0\\\\source\"\n",
    "gt_folder = (\n",
    "    \"U:\\\\DI\\\\MOTUS\\\\Diverse kode tests\\\\BackendV2_0_0\\\\source\\\\output\\\\activity_files\"\n",
    ")\n",
    "res_folder = \"U:\\\\DI\\\\MOTUS\\\\Diverse kode tests\\\\BackendV2_0_0\\\\results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2401', '007', '22004', '22003', '21005']\n"
     ]
    }
   ],
   "source": [
    "# Source files available\n",
    "src_file_ids = list(\n",
    "    set([extract_sensor_id(i) for i in os.listdir(src_folder) if \".bin\" in i])\n",
    ")\n",
    "print(src_file_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f943405c7ffa48a49d1f0bd3a6069f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Ids', layout=Layout(width='200px'), options=('2401', '007', '22004', '22003', '21005'), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516b824393eb40b790857babe39185f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Sensor placements', layout=Layout(width='400px'), options=(), value=())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id_dropdown = widgets.Dropdown(\n",
    "    options=[id for id in src_file_ids],\n",
    "    description=\"Ids\",\n",
    "    layout=widgets.Layout(width=\"200px\"),\n",
    ")\n",
    "placement_dropdown = widgets.SelectMultiple(\n",
    "    description=\"Sensor placements\",\n",
    "    layout=widgets.Layout(width=\"400px\"),\n",
    ")\n",
    "\n",
    "\n",
    "def update(*args):\n",
    "    # Select ID and body placements\n",
    "    selected_id = id_dropdown.value\n",
    "    id_placements = extract_id_placements(selected_id, src_folder)\n",
    "    placement_dropdown.options = list(id_placements.keys())\n",
    "\n",
    "\n",
    "id_dropdown.observe(update)\n",
    "\n",
    "display(id_dropdown)\n",
    "display(placement_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 2401\n",
      "Selected sensors: thigh\n",
      "\n",
      "\tReading thigh ...\n",
      "\tData for thigh succesfully read\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get ID and sensor placements\n",
    "selected_id = id_dropdown.value\n",
    "\n",
    "selected_placements = placement_dropdown.value\n",
    "\n",
    "selected_files = {\n",
    "    key: os.path.join(src_folder,value)\n",
    "    for key, value\n",
    "    in extract_id_placements(selected_id, src_folder).items()\n",
    "    if key in selected_placements\n",
    "}\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "print(f\"ID: {selected_id}\")\n",
    "print(f\"Selected sensors: {', '.join(selected_placements)}\\n\")\n",
    "# Allocate data to data dict\n",
    "for plc, file in selected_files.items():\n",
    "    print(f\"\\tReading {plc} ...\")\n",
    "    data_dict[plc] = read_bin(os.path.join(src_folder, file))\n",
    "    if data_dict[plc] is None:\n",
    "        print(f\"\\tError in reading data for {plc}\\n\")\n",
    "    else:\n",
    "        print(f\"\\tData for {plc} succesfully read\\n\")\n",
    "# name = os.path.join(src_folder, f\"export_raw_{selected_id}.bin\")\n",
    "# gt_file = os.path.join(gt_folder, f\"Activity_perSecond_{selected_id}.csv\")\n",
    "\n",
    "# # Read the bin file\n",
    "# read_bin_data = read_bin(name)\n",
    "\n",
    "# if read_bin_data is None:\n",
    "#     print(\"Error: No data was returned from the read_bin function\\n\")\n",
    "#     sys.exit()\n",
    "# else:\n",
    "#     print(\"Success: Data was returned from the read_bin function\\n\")\n",
    "#     print(\"Sensor ID: \", extract_sensor_id(name), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Ground truth\"-file: Activity_perSecond_2401.csv\n"
     ]
    }
   ],
   "source": [
    "# Search all activity files in ground truth folder\n",
    "possible_files = [\n",
    "    file for file\n",
    "    in os.listdir(gt_folder)\n",
    "    if (\n",
    "        selected_id in file and\n",
    "        \"activity_persecond\" in file.lower()\n",
    "    )\n",
    "]\n",
    "\n",
    "# If only one file matches name format and has ID, choose this\n",
    "if len(possible_files) == 1:\n",
    "    gt_file = os.path.join(gt_folder,possible_files[0])\n",
    "    print(f'\"Ground truth\"-file: {possible_files[0]}')\n",
    "# If more than one file matches activity file name-type and has ID, make user select\n",
    "elif len(possible_files) > 1:\n",
    "    file_list = \"\\n\".join(possible_files)\n",
    "    print(f'Multiple files could be \"ground truth\". Manually put the name of the right one: \\n{file_list}')\n",
    "    file_name = input(\"Copy file name here: \")\n",
    "    gt_file = os.path.join(gt_folder,file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mo5EVz_a09y_"
   },
   "source": [
    "### **3.2 Loading provided dataset - bulk**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7377,
     "status": "ok",
     "timestamp": 1700484090597,
     "user": {
      "displayName": "Amin Rezaei",
      "userId": "03247382606058045083"
     },
     "user_tz": -60
    },
    "id": "yIYDO2ba0-QP",
    "outputId": "fa043d62-7b75-4d4e-aba8-29e40b34135a"
   },
   "outputs": [],
   "source": [
    "# # Create empty lists to store data and timestamps\n",
    "# data_list = []\n",
    "# ts_list = []\n",
    "\n",
    "# # Loop over placement order and append data if any\n",
    "# for key in key_order:\n",
    "#     if key in data_dict:\n",
    "#         data_list.append(data_dict[key][:, 1:4])\n",
    "#         ts_list.append(data_dict[key][:, 0])\n",
    "#     else:\n",
    "#         data_list.append(None)\n",
    "#         ts_list.append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.1 Chunking the Dataset**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 2401\n",
      "Selected sensors: thigh\n",
      "\n",
      "\tChunking data for thigh...\n",
      "\tTotal number of chunks: 5\n",
      "\tStart of first chunk: 2023-10-05 22:00:00\n",
      "\tStart of last chunk: 2023-10-07 11:50:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunk_of_data_in_hours = 12  # in hours\n",
    "overlap_threshold_in_hours = 10  # in minutes\n",
    "\n",
    "# Define the order of data\n",
    "key_order = [\"thigh\", \"trunk\", \"arm\", \"calf\"]\n",
    "\n",
    "chunk_dict = {}\n",
    "\n",
    "print(f\"ID: {selected_id}\")\n",
    "print(f\"Selected sensors: {', '.join(selected_placements)}\\n\")\n",
    "for plc in key_order:\n",
    "    if plc in data_dict.keys():\n",
    "        chunk_dict[plc] = {}\n",
    "        print(f\"\\tChunking data for {plc}...\")\n",
    "        # Process the data\n",
    "        (\n",
    "            chunk_dict[plc][\"list_of_df\"],\n",
    "            chunk_dict[plc][\"sampling_frequency\"],\n",
    "            chunk_dict[plc][\"df\"],\n",
    "            chunk_dict[plc][\"extended_chunks_original_format\"],\n",
    "        ) = chunk_data_extended(\n",
    "            data_dict[plc],\n",
    "            desired_chunking=chunk_of_data_in_hours,\n",
    "            overlapping_threshold=overlap_threshold_in_hours,\n",
    "        )\n",
    "        print(\n",
    "            f\"\\tTotal number of chunks: {len(chunk_dict[plc]['extended_chunks_original_format'])}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\tStart of first chunk: {chunk_dict[plc]['list_of_df'][0].iloc[0,0].strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\tStart of last chunk: {chunk_dict[plc]['list_of_df'][-1].iloc[0,0].strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "        )\n",
    "# # Generate the PDF report\n",
    "# generate_pdf(list_of_df, sampling_frequency, df, name)\n",
    "\n",
    "# ts_list_chunk = []\n",
    "# data_list_chunk = []\n",
    "\n",
    "# out_pre_all_chunks = []\n",
    "# acti4pre_df_all_chunks = []\n",
    "\n",
    "# for i in range(len(extended_chunks_original_format)):\n",
    "#     ts_list_chunk.append(extended_chunks_original_format[i][:, 0])\n",
    "#     data_list_chunk.append(extended_chunks_original_format[i][:, 1:4])\n",
    "\n",
    "# del read_bin_data, ts_list, data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# align chunks on timestamp\n",
    "chunk_dict, chunk_range = get_chunk_start_idx(chunk_dict, selected_placements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. alg_chunk_acti4pre_v1_2_0**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.1 Sanity check on the provided dataset and runing the pre-step on each chunk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_dict = {}\n",
    "for plc in selected_placements:\n",
    "    pre_dict[plc] = {\n",
    "        \"out_pre_all_chunks\": [],\n",
    "        \"motuspre_df_all_chunks\": [],\n",
    "        \"motuspre_df_all_chunks_without_threshold\": [],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70816ace581a42e68cd1ecdafdeec02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running pre step:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "iterprod = product(selected_placements,range(len(chunk_range)))\n",
    "\n",
    "pbar = tqdm(\n",
    "    list(iterprod),\n",
    "    desc = \"Running pre step\",\n",
    ")\n",
    "\n",
    "for plc, i in pbar:\n",
    "    if chunk_dict[plc]['list_of_df'][i] is not None:\n",
    "        ts_list = chunk_dict[plc][\"extended_chunks_original_format\"][i][:, 0]\n",
    "        data_list = chunk_dict[plc][\"extended_chunks_original_format\"][i][:, 1:4]\n",
    "\n",
    "        ts_list = time_sanity(ts_list)\n",
    "\n",
    "        # if len(ts_list) != len(data_list):\n",
    "        #     print(\"ERROR: Data and timestamp lists are not equal in length!\")\n",
    "        # else:\n",
    "        #     print(\n",
    "        #         \"Data and timestamp lists are equal in length, proceeding with the preprocessing...\"\n",
    "        #     )\n",
    "\n",
    "        # perform sanity check on the inputs\n",
    "        ts_list, data_list = sanity_check_pre(ts_list, data_list)\n",
    "\n",
    "        # Create an instance of the ChunkActi4Pre_v1_3_3 class\n",
    "        chunk_acti4_pre = ChunkMotusPre_v2_0_0\n",
    "\n",
    "        # Call the analyse_data_list_new method and pass the inputs\n",
    "        out_ts, out_cat, out_val, out_ver = chunk_acti4_pre.analyse_data_list_new(\n",
    "            ts_list, data_list\n",
    "        )\n",
    "\n",
    "        # combine the categorical and value outputs for step 2\n",
    "        out_pre = np.column_stack((out_ts, out_cat, out_val))\n",
    "\n",
    "        \n",
    "        # save the categorical and value outputs in a list\n",
    "        pre_dict[plc][\"out_pre_all_chunks\"].append(out_pre)\n",
    "\n",
    "        # save the timestamp and categorical/value outputs as a CSV file\n",
    "        motuspre_df = motuspre_merge_outputs(\n",
    "            out_ts, out_cat, out_val, i\n",
    "        )\n",
    "\n",
    "        pre_dict[plc][\"motuspre_df_all_chunks\"].append(motuspre_df)\n",
    "\n",
    "    else:\n",
    "        pre_dict[plc][\"out_pre_all_chunks\"].append(None)\n",
    "        pre_dict[plc][\"motuspre_df_all_chunks\"].append(None)\n",
    "    \n",
    "\n",
    "# del motuspre_df, out_pre           \n",
    "#     ts_list_chunk.append(extended_chunks_original_format[i][:, 0])\n",
    "#     data_list_chunk.append(extended_chunks_original_format[i][:, 1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2 Removing the overlapping minutes and storing chunks**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6915654a0f149808aec1b086a42531c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Storing pre steps:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterprod = product(selected_placements,range(len(chunk_range)))\n",
    "\n",
    "pbar = tqdm(\n",
    "    list(iterprod),\n",
    "    desc = \"Storing pre steps\"\n",
    ")\n",
    "\n",
    "for plc, i in pbar:\n",
    "    if chunk_dict[plc]['list_of_df'][i] is not None:\n",
    "        rows_to_remove = overlap_threshold_in_hours * 60\n",
    "\n",
    "        if i == 0:\n",
    "            pre_dict[plc][\"motuspre_df_all_chunks_without_threshold\"].append(\n",
    "                pre_dict[plc][\"motuspre_df_all_chunks\"][i][: -rows_to_remove + 2]\n",
    "            )\n",
    "\n",
    "        elif i == len(chunk_range)-1:\n",
    "            pre_dict[plc][\"motuspre_df_all_chunks_without_threshold\"].append(\n",
    "                pre_dict[plc][\"motuspre_df_all_chunks\"][i][rows_to_remove:]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            pre_dict[plc][\"motuspre_df_all_chunks_without_threshold\"].append(\n",
    "                pre_dict[plc][\"motuspre_df_all_chunks\"][i][rows_to_remove : -rows_to_remove + 2]\n",
    "            )\n",
    "\n",
    "        if not os.path.exists(\n",
    "            os.path.join(res_folder, f\"{selected_id}/prestep\")\n",
    "        ):\n",
    "            os.makedirs(\n",
    "                os.path.join(res_folder, f\"{selected_id}/prestep\"),\n",
    "                exist_ok=True,\n",
    "            )\n",
    "\n",
    "        pre_dict[plc][\"motuspre_df_all_chunks_without_threshold\"][i].to_csv(\n",
    "            os.path.join(\n",
    "                res_folder,\n",
    "                f\"{selected_id}/prestep\",\n",
    "                f\"pre_chunk_{plc}_{i+1}.csv\"\n",
    "            ),\n",
    "            index=False            \n",
    "        )\n",
    "    else:\n",
    "        pre_dict[plc][\"motuspre_df_all_chunks_without_threshold\"].append(\n",
    "            None\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **4.3 ChunkActi4Pre_v1_2_0 output sanity check**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7201 entries, 0 to 7200\n",
      "Data columns (total 19 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   ts_chunked      7201 non-null   int64 \n",
      " 1   ts_chunked_iso  7201 non-null   object\n",
      " 2   out_cat         7201 non-null   int32 \n",
      " 3   Stdx            7201 non-null   int32 \n",
      " 4   Stdy            7201 non-null   int32 \n",
      " 5   Stdz            7201 non-null   int32 \n",
      " 6   Meanx           7201 non-null   int32 \n",
      " 7   Meany           7201 non-null   int32 \n",
      " 8   Meanz           7201 non-null   int32 \n",
      " 9   hlratio         7201 non-null   int32 \n",
      " 10  Iws             7201 non-null   int32 \n",
      " 11  Irun            7201 non-null   int32 \n",
      " 12  NonWear         7201 non-null   int32 \n",
      " 13  xsum            7201 non-null   int32 \n",
      " 14  zsum            7201 non-null   int32 \n",
      " 15  xSqsum          7201 non-null   int32 \n",
      " 16  zSqsum          7201 non-null   int32 \n",
      " 17  xzsum           7201 non-null   int32 \n",
      " 18  SF12            7201 non-null   int32 \n",
      "dtypes: int32(17), int64(1), object(1)\n",
      "memory usage: 590.8+ KB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43201 entries, 600 to 43800\n",
      "Data columns (total 19 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   ts_chunked      43201 non-null  int64 \n",
      " 1   ts_chunked_iso  43201 non-null  object\n",
      " 2   out_cat         43201 non-null  int32 \n",
      " 3   Stdx            43201 non-null  int32 \n",
      " 4   Stdy            43201 non-null  int32 \n",
      " 5   Stdz            43201 non-null  int32 \n",
      " 6   Meanx           43201 non-null  int32 \n",
      " 7   Meany           43201 non-null  int32 \n",
      " 8   Meanz           43201 non-null  int32 \n",
      " 9   hlratio         43201 non-null  int32 \n",
      " 10  Iws             43201 non-null  int32 \n",
      " 11  Irun            43201 non-null  int32 \n",
      " 12  NonWear         43201 non-null  int32 \n",
      " 13  xsum            43201 non-null  int32 \n",
      " 14  zsum            43201 non-null  int32 \n",
      " 15  xSqsum          43201 non-null  int32 \n",
      " 16  zSqsum          43201 non-null  int32 \n",
      " 17  xzsum           43201 non-null  int32 \n",
      " 18  SF12            43201 non-null  int32 \n",
      "dtypes: int32(17), int64(1), object(1)\n",
      "memory usage: 3.5+ MB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43201 entries, 600 to 43800\n",
      "Data columns (total 19 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   ts_chunked      43201 non-null  int64 \n",
      " 1   ts_chunked_iso  43201 non-null  object\n",
      " 2   out_cat         43201 non-null  int32 \n",
      " 3   Stdx            43201 non-null  int32 \n",
      " 4   Stdy            43201 non-null  int32 \n",
      " 5   Stdz            43201 non-null  int32 \n",
      " 6   Meanx           43201 non-null  int32 \n",
      " 7   Meany           43201 non-null  int32 \n",
      " 8   Meanz           43201 non-null  int32 \n",
      " 9   hlratio         43201 non-null  int32 \n",
      " 10  Iws             43201 non-null  int32 \n",
      " 11  Irun            43201 non-null  int32 \n",
      " 12  NonWear         43201 non-null  int32 \n",
      " 13  xsum            43201 non-null  int32 \n",
      " 14  zsum            43201 non-null  int32 \n",
      " 15  xSqsum          43201 non-null  int32 \n",
      " 16  zSqsum          43201 non-null  int32 \n",
      " 17  xzsum           43201 non-null  int32 \n",
      " 18  SF12            43201 non-null  int32 \n",
      "dtypes: int32(17), int64(1), object(1)\n",
      "memory usage: 3.5+ MB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43201 entries, 600 to 43800\n",
      "Data columns (total 19 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   ts_chunked      43201 non-null  int64 \n",
      " 1   ts_chunked_iso  43201 non-null  object\n",
      " 2   out_cat         43201 non-null  int32 \n",
      " 3   Stdx            43201 non-null  int32 \n",
      " 4   Stdy            43201 non-null  int32 \n",
      " 5   Stdz            43201 non-null  int32 \n",
      " 6   Meanx           43201 non-null  int32 \n",
      " 7   Meany           43201 non-null  int32 \n",
      " 8   Meanz           43201 non-null  int32 \n",
      " 9   hlratio         43201 non-null  int32 \n",
      " 10  Iws             43201 non-null  int32 \n",
      " 11  Irun            43201 non-null  int32 \n",
      " 12  NonWear         43201 non-null  int32 \n",
      " 13  xsum            43201 non-null  int32 \n",
      " 14  zsum            43201 non-null  int32 \n",
      " 15  xSqsum          43201 non-null  int32 \n",
      " 16  zSqsum          43201 non-null  int32 \n",
      " 17  xzsum           43201 non-null  int32 \n",
      " 18  SF12            43201 non-null  int32 \n",
      "dtypes: int32(17), int64(1), object(1)\n",
      "memory usage: 3.5+ MB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35999 entries, 600 to 36598\n",
      "Data columns (total 19 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   ts_chunked      35999 non-null  int64 \n",
      " 1   ts_chunked_iso  35999 non-null  object\n",
      " 2   out_cat         35999 non-null  int32 \n",
      " 3   Stdx            35999 non-null  int32 \n",
      " 4   Stdy            35999 non-null  int32 \n",
      " 5   Stdz            35999 non-null  int32 \n",
      " 6   Meanx           35999 non-null  int32 \n",
      " 7   Meany           35999 non-null  int32 \n",
      " 8   Meanz           35999 non-null  int32 \n",
      " 9   hlratio         35999 non-null  int32 \n",
      " 10  Iws             35999 non-null  int32 \n",
      " 11  Irun            35999 non-null  int32 \n",
      " 12  NonWear         35999 non-null  int32 \n",
      " 13  xsum            35999 non-null  int32 \n",
      " 14  zsum            35999 non-null  int32 \n",
      " 15  xSqsum          35999 non-null  int32 \n",
      " 16  zSqsum          35999 non-null  int32 \n",
      " 17  xzsum           35999 non-null  int32 \n",
      " 18  SF12            35999 non-null  int32 \n",
      "dtypes: int32(17), int64(1), object(1)\n",
      "memory usage: 2.9+ MB\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iterprod = product(selected_placements,range(len(chunk_range)))\n",
    "\n",
    "for plc, i in iterprod:\n",
    "    # print(\n",
    "    #     f\"\\Sanity checking of chunk {i+1} of {len(acti4pre_df_all_chunks_without_threshold)}\\n\"\n",
    "    # )\n",
    "    if pre_dict[plc][\"motuspre_df_all_chunks_without_threshold\"][i] is not None:\n",
    "        display_dataframe_info(pre_dict[plc][\"motuspre_df_all_chunks_without_threshold\"][i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j, i in enumerate(acti4pre_df_all_chunks_without_threshold):\n",
    "#     if j >= 1:\n",
    "#         print(\n",
    "#             f'Last chunk j-1: {acti4pre_df_all_chunks_without_threshold[j-1][\"ts_chunked_iso\"].iloc[-1]}'\n",
    "#         )\n",
    "#         print(f'First chunk j: {i[\"ts_chunked_iso\"].iloc[0]}')\n",
    "#         last_dt = pd.to_datetime(\n",
    "#             acti4pre_df_all_chunks_without_threshold[j - 1][\"ts_chunked_iso\"].iloc[-20:]\n",
    "#         )\n",
    "#         first_dt = pd.to_datetime(i[\"ts_chunked_iso\"].iloc[:20])\n",
    "#         print(\n",
    "#             f'Freq on last 20 (j-1): {np.diff(last_dt).astype(\"timedelta64[ms]\").mean()}'\n",
    "#         )\n",
    "#         print(\n",
    "#             f'Freq on first 20 (j): {np.diff(first_dt).astype(\"timedelta64[ms]\").mean()}'\n",
    "#         )\n",
    "#         print(\n",
    "#             f\"Diff in collection point: {int(abs(first_dt.iloc[0]-last_dt.iloc[-1]).total_seconds()*1000)} miliseconds\"\n",
    "#         )\n",
    "#         print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJxXaDiuTBqt"
   },
   "source": [
    "## **5. alg_activity_acti4_v1_2_0.py**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **5.1 Step2 Preparation**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert chunked data to arrays\n",
    "for plc in selected_placements:\n",
    "    pre_dict[plc][\"out_pre_all_chunks_arrays\"] = process_pre_step_data(pre_dict[plc][\"out_pre_all_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start Timestamp</th>\n",
       "      <th>End Timestamp</th>\n",
       "      <th>Chunk number</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-10-05 12:00:00</td>\n",
       "      <td>2023-10-06 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-10-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-10-06 00:00:00</td>\n",
       "      <td>2023-10-06 12:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-10-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-10-06 12:00:00</td>\n",
       "      <td>2023-10-07 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-10-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-10-07 00:00:00</td>\n",
       "      <td>2023-10-07 12:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-10-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-10-07 12:00:00</td>\n",
       "      <td>2023-10-08 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-10-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Start Timestamp       End Timestamp  Chunk number        Date\n",
       "0 2023-10-05 12:00:00 2023-10-06 00:00:00             1  2023-10-05\n",
       "1 2023-10-06 00:00:00 2023-10-06 12:00:00             2  2023-10-06\n",
       "2 2023-10-06 12:00:00 2023-10-07 00:00:00             3  2023-10-06\n",
       "3 2023-10-07 00:00:00 2023-10-07 12:00:00             4  2023-10-07\n",
       "4 2023-10-07 12:00:00 2023-10-08 00:00:00             5  2023-10-07"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create df of starts and ends of chunks\n",
    "start_end_df = pd.DataFrame(chunk_range,columns=[\"Start Timestamp\"])\n",
    "start_end_df[\"End Timestamp\"] = start_end_df[\"Start Timestamp\"] + pd.Timedelta(hours=12)\n",
    "start_end_df[\"Chunk number\"] = range(1,len(chunk_range)+1)\n",
    "start_end_df[\"Date\"] = start_end_df[\"Start Timestamp\"].dt.date\n",
    "start_end_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "daychunks = list(start_end_df.groupby('Date')['Chunk number'].apply(list).values)\n",
    "days = list(start_end_df['Date'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **5.2 Run step2 on all chunks**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e437b3f113ee4e409c1f42d19d705a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step 2:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/05/23 chunks = [1]\n",
      "10/06/23 chunks = [2, 3]\n",
      "10/07/23 chunks = [4, 5]\n"
     ]
    }
   ],
   "source": [
    "# Define the order of data\n",
    "key_order = [\"thigh\", \"trunk\", \"arm\", \"calf\"]\n",
    "# Find number of cols in data\n",
    "for i in pre_dict[list(pre_dict.keys())[0]]['out_pre_all_chunks']:\n",
    "    if i is not None:\n",
    "        ncols = i.shape[1]\n",
    "        break\n",
    "\n",
    "list_of_dfs = []\n",
    "\n",
    "iterlist = list(zip(daychunks,days))\n",
    "\n",
    "pbar = tqdm(\n",
    "    iterlist,\n",
    "    desc=\"Step 2\",\n",
    "    total=len(iterlist),\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\") # deactivate warnings temprorarily\n",
    "\n",
    "# Loop over chunks related to each day\n",
    "for chunks, day in pbar:\n",
    "    data_list = [] # List to store data for step 2\n",
    "    ts_list = [] # List to store timestamps for step 2\n",
    "    # Add previous chunk (not sure if this should be done)\n",
    "    # if 1 not in chunks:\n",
    "    #     chunks = [min(chunks)-1]+chunks\n",
    "    pbar.write(f\"{day:%D} {chunks = }\")\n",
    "    for plcidx, plc in enumerate(key_order): # loop over placements in correct order\n",
    "        list_of_arrays = [] # List to store data for each chunk\n",
    "        if plc in pre_dict.keys(): # If placement is loaded\n",
    "            for chunk in chunks: # append data for each chunk\n",
    "                if pre_dict[plc][\"out_pre_all_chunks\"][chunk-1] is not None:\n",
    "                    list_of_arrays.append(\n",
    "                        pre_dict[plc][\"out_pre_all_chunks_arrays\"][chunk-1]\n",
    "                    )\n",
    "                else: # and if no data in chunk append None\n",
    "                    list_of_arrays.append(\n",
    "                        [[None]*ncols]\n",
    "                    )\n",
    "        else: # If placement is not selected append None\n",
    "            for chunk in chunks:\n",
    "                list_of_arrays.append(\n",
    "                    [[None]*ncols]\n",
    "                )\n",
    "        conc_array = np.concatenate(list_of_arrays, axis=0) # Concatenate chunks\n",
    "        conc_array = conc_array[(conc_array != None).all(axis=1)] # Remove Nones\n",
    "        data_list.append(conc_array[:,1:] if conc_array.shape[0] != 0 else None)  # Append data for date chunks\n",
    "        ts_list.append(conc_array[:,0])  # Append timestamps for date chunks\n",
    "\n",
    "    \n",
    "    \n",
    "    # Run step 2\n",
    "    activity_motus = ActivityMotus_v1_2_0\n",
    "    parameters = {\"TrunkRef\": None}\n",
    "    \n",
    "    # Call the analyse_data_list_new method and pass the inputs\n",
    "    (\n",
    "        out_ts_step2,\n",
    "        out_cat_step2,\n",
    "        out_val_step2,\n",
    "        out_ver_step2,\n",
    "        _,\n",
    "    ) = activity_motus.analyse_data_list_new(\n",
    "        ts_list,\n",
    "        data_list,\n",
    "        parameters=parameters,\n",
    "        debug_stream=None,\n",
    "        debug_chunks=None,\n",
    "    )\n",
    "\n",
    "    out_ts_step2 = out_ts_step2.reshape(-1,1)\n",
    "    out_cat_step2 = out_cat_step2.reshape(-1,1)\n",
    "    \n",
    "    # combine the categorical and value outputs for step 2\n",
    "    activity_motus = activity_motus_csv(\n",
    "        out_ts_step2, out_cat_step2, out_val_step2, day, selected_id, res_folder, ver=\"thighonly\"\n",
    "    )\n",
    "\n",
    "    activity_motus.drop([\"out_ts\"], axis=1, inplace=True)\n",
    "\n",
    "    # rename the out_ts_iso to out_ts\n",
    "    activity_motus.rename(columns={\"out_ts_iso\": \"out_ts\"}, inplace=True)\n",
    "\n",
    "    list_of_dfs.append(activity_motus)\n",
    "\n",
    "warnings.resetwarnings() # reactivate warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Steps'], dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activity_motus_all.columns[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate output dataframes\n",
    "activity_motus_all = pd.concat(list_of_dfs).reset_index(drop=True)\n",
    "# Remove timezone\n",
    "activity_motus_all[\"out_ts\"] = pd.to_datetime(\n",
    "    activity_motus_all[\"out_ts\"]\n",
    ").dt.tz_localize(None)\n",
    "\n",
    "# Capture value columns (that are multiplied by 1000)\n",
    "val_cols = activity_motus_all.columns[1:-1]\n",
    "\n",
    "# Capture nans (converted to large negative number) and convert\n",
    "# Also divide other values by 1000\n",
    "activity_motus_all.loc[:,val_cols] = activity_motus_all[val_cols].apply(lambda x: np.where(x<-21000000,np.nan,x/1000))\n",
    "# activity_motus_all.loc[angle_nans:angle_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>out_cat</th>\n",
       "      <th>Steps</th>\n",
       "      <th>out_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-10-05 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-10-05 22:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-10-05 22:00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-10-05 22:00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-10-05 22:00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172798</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-10-07 21:59:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172799</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-10-07 21:59:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172800</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-10-07 21:59:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172801</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-10-07 21:59:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172802</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-10-07 21:59:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>172803 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        out_cat  Steps              out_ts\n",
       "0             2    0.0 2023-10-05 22:00:00\n",
       "1             2    0.0 2023-10-05 22:00:01\n",
       "2             2    0.0 2023-10-05 22:00:02\n",
       "3             2    0.0 2023-10-05 22:00:03\n",
       "4             2    0.0 2023-10-05 22:00:04\n",
       "...         ...    ...                 ...\n",
       "172798        2    0.0 2023-10-07 21:59:53\n",
       "172799        2    0.0 2023-10-07 21:59:54\n",
       "172800        2    0.0 2023-10-07 21:59:55\n",
       "172801        2    0.0 2023-10-07 21:59:56\n",
       "172802        2    0.0 2023-10-07 21:59:57\n",
       "\n",
       "[172803 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activity_motus_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **5.3ActivityActi4_v1_3_3 output sanity check**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 172803 entries, 0 to 172802\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count   Dtype         \n",
      "---  ------   --------------   -----         \n",
      " 0   out_cat  172803 non-null  int64         \n",
      " 1   Steps    172803 non-null  float64       \n",
      " 2   out_ts   172803 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(1), int64(1)\n",
      "memory usage: 4.0 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Dataframe Type': pandas.core.frame.DataFrame,\n",
       " 'Dataframe Shape': (172803, 3),\n",
       " 'Dataframe Columns': ['out_cat', 'Steps', 'out_ts'],\n",
       " 'Dataframe Info': None}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analyse the output and data attributes\n",
    "display_dataframe_info(activity_motus_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Plot results** (muted for now)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here , but comment out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Validating results**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **7.1. Loading ground truth data and ranging it to match the chunk begining and ending**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute match stats\n",
    "def match_stats(\n",
    "    df_in, match_var=\"match\", perday=True, date_var=\"out_ts\", print_res=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze match statistics from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df_in (DataFrame): Input DataFrame containing match data.\n",
    "    - match_var (str, optional): Column name in the DataFrame containing match information. Defaults to 'match'.\n",
    "    - perday (bool, optional): Indicates whether to analyze match statistics per day. Defaults to True.\n",
    "    - date_var (str, optional): Column name in the DataFrame containing dates. Defaults to 'out_ts'.\n",
    "    - print_res (bool, optional): Indicates whether to print the analysis results. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing match statistics DataFrames.\n",
    "        - 'collapsed': DataFrame summarizing match counts and proportions.\n",
    "        - 'perday' (if perday is True): DataFrame summarizing match counts and proportions per day.\n",
    "\n",
    "    Notes:\n",
    "    - If perday is True and date_var is None, it raises an error.\n",
    "    - If perday is True and date_var is not found in the DataFrame columns, it sets perday to False.\n",
    "    \"\"\"\n",
    "    if perday and date_var is None:\n",
    "        print(\"date_var must be defined\")\n",
    "        return None\n",
    "    elif perday and date_var not in df_in.columns:\n",
    "        print(f\"{date_var} not in coluns of inputted dataframe\")\n",
    "        perday = False\n",
    "\n",
    "    df_out = {}\n",
    "    # Create a DataFrame with counts and proportions\n",
    "    df_out[\"collapsed\"] = pd.DataFrame(\n",
    "        {\n",
    "            \"count\": df_in[match_var].value_counts(),\n",
    "            \"proportion %\": df_in[match_var].value_counts(normalize=True) * 100,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    msg = \"\"\n",
    "    msg += \"-\" * 40\n",
    "    msg += \"\\n\"\n",
    "    msg += \"Matching results \\n\"\n",
    "    msg += \"-\" * 40\n",
    "    msg += \"\\n\\n\"\n",
    "    msg += df_out[\"collapsed\"].to_string()\n",
    "\n",
    "    if perday:\n",
    "        # Create a DataFrame with counts and proportions\n",
    "        df_out[\"perday\"] = pd.DataFrame(\n",
    "            {\n",
    "                \"count\": df_in.groupby(df_in[date_var].dt.date)[\n",
    "                    match_var\n",
    "                ].value_counts(),\n",
    "                \"proportion %\": df_in.groupby(df_in[date_var].dt.date)[\n",
    "                    match_var\n",
    "                ].value_counts(normalize=True)\n",
    "                * 100,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        msg += \"\\n\\n\"\n",
    "        msg += \"-\" * 40\n",
    "        msg += \"\\n\\n\"\n",
    "        msg += df_out[\"perday\"].to_string()\n",
    "\n",
    "    if print_res:\n",
    "        print(msg)\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1         False\n",
       "2         False\n",
       "3         False\n",
       "4         False\n",
       "          ...  \n",
       "691194    False\n",
       "691195    False\n",
       "691196    False\n",
       "691197    False\n",
       "691198    False\n",
       "Name: out_ts, Length: 691199, dtype: bool"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((df_gt['out_ts'] >= start_end_timestamps[0]) & \n",
    " (df_gt['out_ts'] <= start_end_timestamps[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison for timeperiod: \n",
      "   2023-10-05 - 2023-10-07\n",
      "----------------------------------------\n",
      "Matching results \n",
      "----------------------------------------\n",
      "\n",
      "        count  proportion %\n",
      "match                      \n",
      "True   164780     95.360452\n",
      "False    8017      4.639548\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "                  count  proportion %\n",
      "out_ts_gt  match                     \n",
      "2023-10-05 True    4919     68.319444\n",
      "           False   2281     31.680556\n",
      "2023-10-06 True   84932     98.300926\n",
      "           False   1468      1.699074\n",
      "2023-10-07 True   74929     94.610907\n",
      "           False   4268      5.389093\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This section performs the following tasks:\n",
    "1. Takes the 'activity_acti4' dataframe and extracts the first and last timestamps from the 'out_ts' column, saving them in a list.\n",
    "2. Loads a CSV file ('Activity_perSecond_2401.csv') as a dataframe ('df_gt').\n",
    "3. Selects only the necessary columns ('Time' and 'Activity') from 'df_gt'.\n",
    "4. Renames the columns to match the output dataframe ('out_cat' and 'out_ts').\n",
    "5. Converts the 'out_ts' column to a timezone-aware datetime object, if it's not already.\n",
    "6. Removes the timezone information, converting it to a timezone-naive datetime object.\n",
    "7. Subtracts 8 hours from the 'out_ts' column.\n",
    "8. Filters the 'df_gt' dataframe to include only rows with timestamps between the first and last timestamps from 'activity_acti4'.\n",
    "9. Resets the index of the filtered 'df_gt' dataframe.\n",
    "10. Selects only the necessary columns ('out_ts' and 'out_cat') from 'activity_acti4' and filters it to match the length of 'df_gt_filtered'.\n",
    "11. Selects the columns 'out_ts' and 'out_cat' from 'activity_acti4_filtered'.\n",
    "12. Selects the 'out_cat' column from 'df_gt_filtered' and renames it to 'out_cat_gt' to avoid column name conflicts.\n",
    "13. Concatenates the selected columns horizontally to create a new dataframe ('new_df').\n",
    "14. Compares the 'out_cat' and 'out_cat_gt' columns in 'new_df' and creates a new column called 'match' with True if the values match and False if they don't.\n",
    "15. Creates a dataframe ('match_stats') with counts and proportions of the 'match' column values.\n",
    "16. Prints the 'match_stats' dataframe.\n",
    "\"\"\"\n",
    "\n",
    "# take the activity_acti4 dataframe, go to column out_ts and take the first and last timestamp and save them in a list\n",
    "start_end_timestamps = []\n",
    "\n",
    "# locate the first timestamp in the dataframe and append it to the list for future validation purposes\n",
    "start_end_timestamps.append(activity_motus_all[[\"out_ts\"]].iloc[0].values[0])\n",
    "start_end_timestamps.append(activity_motus_all[[\"out_ts\"]].iloc[-1].values[0])\n",
    "\n",
    "print(\n",
    "    f'Comparison for timeperiod: \\n   {start_end_timestamps[0].astype(\"datetime64[D]\")} - {start_end_timestamps[-1].astype(\"datetime64[D]\")}'\n",
    ")\n",
    "\n",
    "# load csv file as a dataframe\n",
    "df_gt = pd.read_csv(gt_file)\n",
    "\n",
    "# rename the columns to match the output dataframe, Activity to out_cat and Time to out_ts\n",
    "df_gt.rename(columns={\"Activity\": \"out_cat\", \"Time\": \"out_ts\"}, inplace=True)\n",
    "\n",
    "# only use the columns that we need for the comparison, including time and activity\n",
    "keep_cols = ['out_ts', 'out_cat']\n",
    "if 'trunk' in data_dict.keys():\n",
    "    keep_cols += ['TrunkInc', 'TrunkFB']\n",
    "if 'arm' in data_dict.keys():\n",
    "    keep_cols += ['ArmInc']\n",
    "df_gt = df_gt[keep_cols]\n",
    "\n",
    "# rename the Activity column numerical values to the corresponding activity using a dictionary 1 == 'lie' , 2 == 'sit', 3 == 'stand', 4 == 'move', 5 == 'walk', 6 == 'run', 7 == 'stair', 8 == 'cycle', 9 == 'row'\n",
    "df_gt[\"activity_name\"] = df_gt[\"out_cat\"].map(\n",
    "    {\n",
    "        1: \"lie\",\n",
    "        2: \"sit\",\n",
    "        3: \"stand\",\n",
    "        4: \"move\",\n",
    "        5: \"walk\",\n",
    "        6: \"run\",\n",
    "        7: \"stair\",\n",
    "        8: \"cycle\",\n",
    "        9: \"row\",\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Convert 'out_ts' to a timezone-aware datetime object, if it's not already\n",
    "df_gt[\"out_ts\"] = pd.to_datetime(df_gt[\"out_ts\"])\n",
    "\n",
    "# Remove the timezone information, converting to timezone-naive datetime object\n",
    "df_gt[\"out_ts\"] = df_gt[\"out_ts\"].dt.tz_convert(None)\n",
    "\n",
    "# Use only time from gt that it is in backend version\n",
    "df_gt = df_gt[(\n",
    "    (df_gt['out_ts'] >= start_end_timestamps[0]) &\n",
    "    (df_gt['out_ts'] <= start_end_timestamps[1]) \n",
    ")]\n",
    "\n",
    "# Selecting columns from backend DataFrame\n",
    "activity_motus_cols = activity_motus_all[keep_cols]\n",
    "# Sort on time (necessary for pd.merge_asof())\n",
    "activity_motus_cols = activity_motus_cols.sort_values(\"out_ts\")\n",
    "# Shift backend time (seems necessary for now)\n",
    "time_shift = pd.Timedelta(seconds=1)\n",
    "activity_motus_cols[\"out_ts\"] += time_shift\n",
    "\n",
    "# Selecting the column from ground truth DataFrame and renaming it to avoid conflict\n",
    "df_gt_cols = df_gt.rename(columns={col: f\"{col}_gt\" for col in df_gt.columns})\n",
    "\n",
    "# Merge ground truth classifications with backend classifications based on nearest timestamps\n",
    "new_df = pd.merge_asof(\n",
    "    left=df_gt_cols,\n",
    "    right=activity_motus_cols,\n",
    "    left_on=\"out_ts_gt\",\n",
    "    right_on=\"out_ts\",\n",
    "    direction=\"nearest\",\n",
    "    tolerance=pd.Timedelta(seconds=1),\n",
    ")\n",
    "\n",
    "# compare the out_cat and out_cat_gt columns and create a new column called 'match' which will be True if the values match and False if they don't\n",
    "new_df[\"match\"] = new_df[\"out_cat\"] == new_df[\"out_cat_gt\"]\n",
    "\n",
    "match_res = match_stats(\n",
    "    new_df, match_var=\"match\", perday=True, date_var=\"out_ts_gt\", print_res=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          True\n",
       "1          True\n",
       "2          True\n",
       "3          True\n",
       "4          True\n",
       "          ...  \n",
       "172792    False\n",
       "172793    False\n",
       "172794    False\n",
       "172795    False\n",
       "172796    False\n",
       "Name: out_ts_gt, Length: 172797, dtype: bool"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[\"out_ts_gt\"].dt.date.astype(str) == \"2023-10-05\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>out_ts_gt</th>\n",
       "      <th>out_cat_gt</th>\n",
       "      <th>activity_name_gt</th>\n",
       "      <th>out_ts</th>\n",
       "      <th>out_cat</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-10-05 22:00:00.027003136</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lie</td>\n",
       "      <td>2023-10-05 22:00:01</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-10-05 22:00:01.027007744</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lie</td>\n",
       "      <td>2023-10-05 22:00:01</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-10-05 22:00:02.027002368</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lie</td>\n",
       "      <td>2023-10-05 22:00:02</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-10-05 22:00:03.027006976</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lie</td>\n",
       "      <td>2023-10-05 22:00:03</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-10-05 22:00:04.027001600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lie</td>\n",
       "      <td>2023-10-05 22:00:04</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172792</th>\n",
       "      <td>2023-10-07 21:59:52.027006208</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lie</td>\n",
       "      <td>2023-10-07 21:59:52</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172793</th>\n",
       "      <td>2023-10-07 21:59:53.027000832</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lie</td>\n",
       "      <td>2023-10-07 21:59:53</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172794</th>\n",
       "      <td>2023-10-07 21:59:54.027005440</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lie</td>\n",
       "      <td>2023-10-07 21:59:54</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172795</th>\n",
       "      <td>2023-10-07 21:59:55.027000064</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lie</td>\n",
       "      <td>2023-10-07 21:59:55</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172796</th>\n",
       "      <td>2023-10-07 21:59:56.027004672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lie</td>\n",
       "      <td>2023-10-07 21:59:56</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>172797 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           out_ts_gt  out_cat_gt activity_name_gt  \\\n",
       "0      2023-10-05 22:00:00.027003136         1.0              lie   \n",
       "1      2023-10-05 22:00:01.027007744         1.0              lie   \n",
       "2      2023-10-05 22:00:02.027002368         1.0              lie   \n",
       "3      2023-10-05 22:00:03.027006976         1.0              lie   \n",
       "4      2023-10-05 22:00:04.027001600         1.0              lie   \n",
       "...                              ...         ...              ...   \n",
       "172792 2023-10-07 21:59:52.027006208         1.0              lie   \n",
       "172793 2023-10-07 21:59:53.027000832         1.0              lie   \n",
       "172794 2023-10-07 21:59:54.027005440         1.0              lie   \n",
       "172795 2023-10-07 21:59:55.027000064         1.0              lie   \n",
       "172796 2023-10-07 21:59:56.027004672         1.0              lie   \n",
       "\n",
       "                    out_ts  out_cat  match  \n",
       "0      2023-10-05 22:00:01        2  False  \n",
       "1      2023-10-05 22:00:01        2  False  \n",
       "2      2023-10-05 22:00:02        2  False  \n",
       "3      2023-10-05 22:00:03        2  False  \n",
       "4      2023-10-05 22:00:04        2  False  \n",
       "...                    ...      ...    ...  \n",
       "172792 2023-10-07 21:59:52        2  False  \n",
       "172793 2023-10-07 21:59:53        2  False  \n",
       "172794 2023-10-07 21:59:54        2  False  \n",
       "172795 2023-10-07 21:59:55        2  False  \n",
       "172796 2023-10-07 21:59:56        2  False  \n",
       "\n",
       "[172797 rows x 6 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "anglecols = [\"TrunkInc\", \"TrunkFB\", \"TrunkLat\", \"ArmInc\"]\n",
    "anglecols = [i for i in anglecols if i in new_df.columns]\n",
    "\n",
    "if len(anglecols) > 0:\n",
    "    \n",
    "    fig_w = 5*len(anglecols)\n",
    "    f, axs = plt.subplots(\n",
    "        nrows=1,\n",
    "        ncols=len(anglecols),\n",
    "        figsize=(fig_w,5),\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "    )\n",
    "    \n",
    "    f.suptitle(\"Histograms of differences\")\n",
    "    \n",
    "    for i, angle in enumerate(anglecols):\n",
    "        ax = axs[i]\n",
    "        (new_df[angle] - new_df[f\"{angle}_gt\"]).plot(kind=\"hist\", bins=100, ax=ax)\n",
    "        stats = (new_df[angle] - new_df[f\"{angle}_gt\"]).describe()[1:]\n",
    "        msg = \"Stats: \\n\"\n",
    "        for i, stat in enumerate(stats):\n",
    "            msg += f\"{stats.index[i]:4} {stat:>7.2f}\\n\"\n",
    "        ax.set_title(angle)\n",
    "        ax.text(\n",
    "            y=ax.get_ylim()[1]*0.95,\n",
    "            x=ax.get_xlim()[0]+abs(ax.get_xlim()[0]*0.05),\n",
    "            s=msg,\n",
    "            family=\"monospace\",\n",
    "            va='top',\n",
    "            ha='left',\n",
    "        )\n",
    "    \n",
    "    \n",
    "    f.tight_layout()\n",
    "    f.savefig(os.path.join(res_folder,selected_id,\"angle_diff_hist.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write statistics of comparison to .txt-file\n",
    "with open(\n",
    "    os.path.join(\n",
    "        res_folder, f\"{selected_id}\\\\match_stats_bulk_V_2_0_0.txt\"\n",
    "    ),\n",
    "    \"w\",\n",
    ") as f:\n",
    "    f.write(\"-\" * 40)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(match_res[\"collapsed\"].to_string())\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"-\" * 40)\n",
    "    f.write(match_res[\"perday\"].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bokeh plot for comparison of ground truth and backend classifications\n",
    "\n",
    "df_plot = new_df\n",
    "\n",
    "output_file(\n",
    "    filename=os.path.join(\n",
    "        res_folder, f\"{extract_sensor_id(name)}\\\\comparison_plot.html\"\n",
    "    )\n",
    ")\n",
    "\n",
    "dates = np.array(df_plot[\"out_ts\"], dtype=np.datetime64)\n",
    "source = ColumnDataSource(\n",
    "    data=dict(date=dates, cat=df_plot[\"out_cat\"], cat_gt=df_plot[\"out_cat_gt\"])\n",
    ")\n",
    "\n",
    "p = figure(\n",
    "    height=300,\n",
    "    width=800,\n",
    "    tools=[\"xpan\", \"xwheel_zoom\"],\n",
    "    toolbar_location=\"above\",\n",
    "    x_axis_type=\"datetime\",\n",
    "    x_axis_location=\"above\",\n",
    "    active_scroll=\"xwheel_zoom\",\n",
    "    background_fill_color=\"#efefef\",\n",
    "    x_range=(dates[1500], dates[2500]),\n",
    ")\n",
    "\n",
    "# p = figure(height=300, width=1000, tools=[\"xpan\",'xwheel_zoom'], toolbar_location='above',\n",
    "#                    x_axis_type=\"datetime\", x_axis_location=\"above\", active_scroll = \"xwheel_zoom\", x_range=(dates[0], dates[slider_end_index]),\n",
    "#                    title=f'Activities and intensity measured on {tmpdag.Time.iloc[0].day_name()} {tmpdag.Time.iloc[0].day}/{tmpdag.Time.iloc[0].month} {tmpdag.Time.iloc[0].year} (scroll to zoom and drag to pan)')\n",
    "\n",
    "\n",
    "p.line(\n",
    "    \"date\", \"cat\", source=source, color=\"orange\", legend_label=\"Backend classifications\"\n",
    ")\n",
    "p.line(\"date\", \"cat_gt\", source=source, legend_label=\"Offline classifications\")\n",
    "p.yaxis.axis_label = \"Categories\"\n",
    "\n",
    "select = figure(\n",
    "    title=\"Drag the middle and edges of the selection box to change the range above\",\n",
    "    height=130,\n",
    "    width=800,\n",
    "    y_range=p.y_range,\n",
    "    x_axis_type=\"datetime\",\n",
    "    y_axis_type=None,\n",
    "    tools=\"\",\n",
    "    toolbar_location=None,\n",
    "    background_fill_color=\"#efefef\",\n",
    ")\n",
    "\n",
    "range_tool = RangeTool(x_range=p.x_range)\n",
    "range_tool.overlay.fill_color = \"navy\"\n",
    "range_tool.overlay.fill_alpha = 0.2\n",
    "\n",
    "select.line(\"date\", \"cat\", source=source, color=\"orange\")\n",
    "select.line(\"date\", \"cat_gt\", source=source)\n",
    "select.ygrid.grid_line_color = None\n",
    "select.add_tools(range_tool)\n",
    "\n",
    "# show(column(p, select))\n",
    "save(column(p, select))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **7.2. Saving the non-matching activities, those that are difference from the GT data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the rows where the values don't match, i.e., where the match column is False only - store it as non_mathing_activity\n",
    "non_matching_activity = new_df[new_df[\"match\"] == False]\n",
    "\n",
    "# save the non_matching_activity dataframe as a csv file\n",
    "non_matching_activity.to_csv(\n",
    "    os.path.join(res_folder, f\"{extract_sensor_id(name)}\\\\non_matching_activity.csv\"),\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "# print where the non_matching_activity dataframe is saved\n",
    "print(\n",
    "    f\"The non_matching_activity dataframe is saved in \",\n",
    "    os.path.join(res_folder, f\"{extract_sensor_id(name)}\\\\non_matching_activity.csv\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **7.3. Ploting side by side (muted for now)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_activity_classification(df_gt_cols, plot_title='Ground Truth Activity Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_activity_classification(activity_acti4_cols, plot_title='Activity Classification - Motus backend v1.2.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. Backend in one go**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import file to run backend in one go\n",
    "from functions.BackendFiles import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run backend classification and store outputs as dataframe\n",
    "Akt_full, Fstep_full, out_val_full, out_ts_full = backend.classify(\n",
    "    read_bin_data[:, 0], read_bin_data[:, 1:4]\n",
    ")\n",
    "backend_full_df = pd.DataFrame(\n",
    "    {\n",
    "        \"out_cat_full\": Akt_full,\n",
    "        \"out_ts_full\": pd.to_datetime(out_ts_full, utc=False, unit=\"ms\"),\n",
    "    }\n",
    ")\n",
    "backend_full_df[\"out_ts_full\"] += time_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with ground truth\n",
    "compare_full = pd.merge_asof(\n",
    "    left=backend_full_df, right=new_df, left_on=\"out_ts_full\", right_on=\"out_ts_gt\"\n",
    ")\n",
    "\n",
    "compare_full[\"match\"] = compare_full[\"out_cat_full\"] == compare_full[\"out_cat_gt\"]\n",
    "\n",
    "match_res_full = match_stats(compare_full, perday=True, date_var=\"out_ts_gt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results in .txt-file\n",
    "with open(\n",
    "    os.path.join(\n",
    "        res_folder, f\"{extract_sensor_id(name)}\\\\match_stats_full_V_1_2_0.txt\"\n",
    "    ),\n",
    "    \"w\",\n",
    ") as f:\n",
    "    f.write(\"-\" * 40)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(match_res_full[\"collapsed\"].to_string())\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"-\" * 40)\n",
    "    f.write(match_res_full[\"perday\"].to_string())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NTt00Y6qGQvF",
    "wxYd-1C4GQvH",
    "VrqMPIovGQvK",
    "qbuEVYP3mDEi"
   ],
   "provenance": [
    {
     "file_id": "1WF6DqJ9ZEjwfw-yif3XlhxcH-mEA4S0S",
     "timestamp": 1700479215642
    },
    {
     "file_id": "1IsXV17fdTtkwwS5J2jQHyra4Z7umUiyr",
     "timestamp": 1700468436401
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0841c12cd28642db8d03b1e035f790ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ed85bab65cc4b76bb9e88e2d9466fb2",
      "placeholder": "​",
      "style": "IPY_MODEL_e7a6f7364d5a474dabf9ff848ea44933",
      "value": " 6/6 [00:59&lt;00:00,  8.77s/it]"
     }
    },
    "0f983cf88f8a48b5a536f4309933d45c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24e54d7672e6421f951d22881f9e8ce9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "699d863e3eaa4facb96731e6a05aff4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6ed85bab65cc4b76bb9e88e2d9466fb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "771e97558331482490c930cf9c82431f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a962e0ae64a84eefab29fa5a3fae7ab2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f983cf88f8a48b5a536f4309933d45c",
      "placeholder": "​",
      "style": "IPY_MODEL_d2cddd8b5fdf42f595e887e85d62df59",
      "value": "Installing in progress: 100%"
     }
    },
    "d2cddd8b5fdf42f595e887e85d62df59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de445b96f566443589278f407676c384": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a962e0ae64a84eefab29fa5a3fae7ab2",
       "IPY_MODEL_fbc5409c394c405798895675424fc70b",
       "IPY_MODEL_0841c12cd28642db8d03b1e035f790ce"
      ],
      "layout": "IPY_MODEL_771e97558331482490c930cf9c82431f"
     }
    },
    "e7a6f7364d5a474dabf9ff848ea44933": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fbc5409c394c405798895675424fc70b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24e54d7672e6421f951d22881f9e8ce9",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_699d863e3eaa4facb96731e6a05aff4c",
      "value": 6
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
